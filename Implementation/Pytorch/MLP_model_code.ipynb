{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import numpy as np  \n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "#from github import Github\n",
    "#import github\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import tensor dataset & data loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Union, Tuple\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "import math\n",
    "import random\n",
    "import imageio\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "matplotlib.style.use('seaborn')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, input_dim, layers,output_dim):\n",
    "    super(MLP, self).__init__()\n",
    "    self.input_dim=input_dim\n",
    "    self.n_layers=layers\n",
    "    self.output_dim=output_dim\n",
    "    in_features=input_dim\n",
    "    out_features=16\n",
    "    layers=[]\n",
    "    for l in range(self.n_layers):\n",
    "        if l==(self.n_layers-1):\n",
    "          layers.append(nn.Linear(in_features=in_features, out_features=self.output_dim))\n",
    "        else:\n",
    "          layers.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "        if l%2==0:\n",
    "          layers.append(nn.Tanh())\n",
    "        else:\n",
    "          layers.append(nn.ELU(inplace=True))\n",
    "        in_features=out_features\n",
    "        out_features=int(out_features/2)\n",
    "    self.body = nn.Sequential(*layers)\n",
    "  def forward(self, x):\n",
    "    b, n_steps, features = x.shape\n",
    "    #print(b,n_steps, features)\n",
    "    x = x.reshape([b,n_steps*features])\n",
    "    return self.body(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shortlisted_States=['Maharashtra','Delhi','Uttar-Pradesh','Kerala','Tamil-Nadu']\n",
    "results_mlp=[]\n",
    "for state in Shortlisted_States:\n",
    "  best_models=[]\n",
    "  df=pd.read_csv(\"https://raw.githubusercontent.com/sureshkuc/Data-Science-in-Life-Science-Project/main/Indian-States-Covid19-Datasets/\"+state+\".csv\", parse_dates=[\"Date\"]).drop(columns =[\"Unnamed: 0\"])\n",
    "  df = df[df[\"Date\"] > \"2020-03-19\"]\n",
    "  df = df.set_index(\"Date\")\n",
    "  df = df[['Confirmed', 'Recovered', 'Deceased', 'New_Confirmerd', 'New_Deaths', 'New_Recovered']]\n",
    "  #print(df.describe())\n",
    "\n",
    "  time_step=[5,7,15,30]\n",
    "  Number_of_feature=[1,2,3,4,5,6]\n",
    "  multi_feature=True\n",
    "  for n_f in Number_of_feature:\n",
    "    for t_s in time_step:\n",
    "      train_loader, test_loader = data_preparation(df, scaling_range=(0,1),time_step=t_s,number_feature=n_f, response_variable_index=0,data_split_ratio=0.8)\n",
    "      for n_layers in range(1,3,1):\n",
    "          print(state,'n_f',n_f,'t_s',t_s,'n_layers',n_layers,'Error',mae,rmse,r2s)\n",
    "          max_epochs=20\n",
    "          random.seed(42)\n",
    "          torch.manual_seed(42)\n",
    "          np.random.seed(42)\n",
    "          #CNN model with L1 loss\n",
    "          #best_model=Call_CNN_model(state,dataset=(train_loader, test_loader), lr=1e-2,criterion=nn.L1Loss(),max_epochs=max_epochs)\n",
    "          fc_model = MLP(input_dim=n_f*t_s, layers=n_layers,output_dim=1)\n",
    "          cuda=torch.cuda.is_available()\n",
    "          if cuda:\n",
    "            fc_model = fc_model.cuda()\n",
    "          fc_optim = optim.SGD(fc_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "          #fc_optim = optim.Adam(fc_model.parameters(), lr=1e-3)\n",
    "          train_losses,test_losses,best_model = fit(fc_model, fc_optim,nn.L1Loss(),(train_loader, test_loader), max_epochs=max_epochs,cuda=cuda)\n",
    "          #print(f'\\nTraining took {end-start}s!')\n",
    "          #plot_loss(max_epochs,train_losses,test_losses,model_name='CNN for '+state)\n",
    "          fc_model = MLP(input_dim=n_f*t_s, layers=n_layers,output_dim=1)\n",
    "          fc_model.load_state_dict(best_model)\n",
    "          fc_model.eval()\n",
    "          test_x,test_y=test_loader\n",
    "          predictions=fc_model(test_x)\n",
    "          test_y=test_y.cpu().detach().numpy()\n",
    "          predictions=predictions.cpu().detach().numpy()\n",
    "          mae=mean_absolute_error(test_y,predictions)\n",
    "          rmse=math.sqrt(mean_squared_error(test_y,predictions))\n",
    "          #mape=mean_absolute_percentage_error(test_y,predictions)\n",
    "          r2s=r2_score(test_y,predictions)\n",
    "          results_mlp.append([state,n_f,t_s,n_layers,mae,rmse,r2s])\n",
    "          #print(state,n_f,t_s,n_layers,mae,rmse,r2s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
