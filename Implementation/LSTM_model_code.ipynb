{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import numpy as np  \n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "#from github import Github\n",
    "#import github\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import tensor dataset & data loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Union, Tuple\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "import math\n",
    "import random\n",
    "import imageio\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "matplotlib.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "#random.seed(42)\n",
    "#torch.manual_seed(42)\n",
    "#np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,  output_dim,num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length=seq_length\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.relu = nn.ELU()\n",
    "        # Readout layer\n",
    "        print(output_dim)\n",
    "        self.fc = nn.Linear(hidden_dim*self.seq_length, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        x = out.contiguous().view(batch_size,-1)\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 32, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(self.relu(x)) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shortlisted_States=['Maharashtra','Delhi','Uttar-Pradesh','Kerala','Tamil-Nadu']\n",
    "results_lstm=[]\n",
    "lstm_models=[]\n",
    "for state in Shortlisted_States:\n",
    "  best_models=[]\n",
    "  df=pd.read_csv(\"https://raw.githubusercontent.com/sureshkuc/Data-Science-in-Life-Science-Project/main/Indian-States-Covid19-Datasets/\"+state+\".csv\", parse_dates=[\"Date\"]).drop(columns =[\"Unnamed: 0\"])\n",
    "  df = df[df[\"Date\"] > \"2020-03-19\"]\n",
    "  df = df.set_index(\"Date\")\n",
    "  df = df[['Confirmed', 'Recovered', 'Deceased', 'New_Confirmerd', 'New_Deaths', 'New_Recovered']]\n",
    "  #print(df.describe())\n",
    "\n",
    "  time_step=[5,7,15]\n",
    "  Number_of_feature=[1,2,3,4,5,6]\n",
    "  multi_feature=True\n",
    "  output_dim=1\n",
    "  min_error=np.iinfo(0).max\n",
    "  lstm_best_model={}\n",
    "  for n_f in Number_of_feature:\n",
    "    for t_s in time_step:\n",
    "      train_loader, test_loader = data_preparation(df, scaling_range=(0,1),time_step=t_s,number_feature=n_f, response_variable_index=0,data_split_ratio=0.8, Suffle=False)\n",
    "      for n_layers in range(1,2,1):\n",
    "        for n_hidden_nodes in [1,5,8,16,32]:\n",
    "          #random.seed(42)\n",
    "          #torch.manual_seed(42)\n",
    "          #np.random.seed(42)\n",
    "          max_epochs=100\n",
    "          \n",
    "          #CNN model with L1 loss\n",
    "          #best_model=Call_CNN_model(state,dataset=(train_loader, test_loader), lr=1e-2,criterion=nn.L1Loss(),max_epochs=max_epochs)\n",
    "          lstm_model = LSTM(n_f, n_hidden_nodes, output_dim, n_layers,t_s)\n",
    "          #if torch.cuda.is_available():\n",
    "          #stm_model = lstm_model.cuda()\n",
    "          #print(lstm_model)\n",
    "          lstm_optim = optim.SGD(lstm_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "          #fc_optim = optim.Adam(fc_model.parameters(), lr=1e-3)\n",
    "          train_losses,test_losses,best_model = fit(lstm_model, lstm_optim,nn.L1Loss(),(train_loader, test_loader), max_epochs=max_epochs,cuda=False)\n",
    "          #print(f'\\nTraining took {end-start}s!')\n",
    "          #plot_loss(max_epochs,train_losses,test_losses,model_name='CNN for '+state)\n",
    "          lstm_model = LSTM(n_f, n_hidden_nodes, output_dim, n_layers,t_s)\n",
    "          lstm_model.load_state_dict(best_model)\n",
    "          lstm_model.eval()\n",
    "          test_x,test_y=test_loader\n",
    "          predictions=lstm_model(test_x)\n",
    "          test_y=test_y.cpu().detach().numpy()\n",
    "          predictions=predictions.cpu().detach().numpy()\n",
    "          mae=mean_absolute_error(test_y,predictions)\n",
    "          rmse=math.sqrt(mean_squared_error(test_y,predictions))\n",
    "          if rmse<min_error:\n",
    "            min_error=rmse\n",
    "            lstm_best_model=best_model\n",
    "          #mape=mean_absolute_percentage_error(test_y,predictions)\n",
    "          r2s=r2_score(test_y,predictions)\n",
    "          results_lstm.append([state,n_f,t_s,n_layers,n_hidden_nodes,mae,rmse,r2s])\n",
    "          print(state,'n_f',n_f,'t_s',t_s,'n_layers',n_layers,n_hidden_nodes,'Error',mae,rmse,r2s)\n",
    "  lstm_models.append(lstm_best_model) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
